{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XenMoji_Main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xentex/XentexProject/blob/master/XenMoji_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ya3geNweLra3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  پروژه تشخیص ایموجی\n",
        "\n",
        "**تحلیل معنایی یا احساسی** \n",
        "\n",
        "**طبقه بندی متن**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TdMGLr66MSAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# افزودن کتابخانه های مورد نیاز"
      ]
    },
    {
      "metadata": {
        "id": "qZUO7eTYpd0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "%matplotlib inline     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IZoB3URpIVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ** دانلود و معرفی دیتاست**\n",
        "\n",
        "**مشاهده لیست ایموجی ها و کد آنها**\n",
        "\n",
        "[Emoji List](https://www.webfx.com/tools/emoji-cheat-sheet/)\n",
        "\n",
        "\n",
        "[Project site](https://pypi.org/project/emoji/)"
      ]
    },
    {
      "metadata": {
        "id": "fWvbq-eTANyl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://dataset.class.vision/NLP/emoji.zip\n",
        "!unzip emoji.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McSpAW4z4VgY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def read_csv(filename):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y\n",
        "  \n",
        "  \n",
        "\n",
        "X_train, Y_train = read_csv('train_emoji.csv')\n",
        "X_test, Y_test = read_csv('tesss.csv')\n",
        "maxLen = len(max(X_train, key=len).split())\n",
        "#maxLen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rI-LDzaspgkZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **تابع کمکی برای تبدیل برچسب ها به ایموجی**"
      ]
    },
    {
      "metadata": {
        "id": "1-B9jgW1yIM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "\n",
        "import emoji \n",
        "\n",
        "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":joy:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "    \"\"\"\n",
        "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    \"\"\"\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
        "  \n",
        "  \n",
        "########## بررسی درستی لیبل ها\n",
        "#index = 5\n",
        "#print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-8TO447yx1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# تبدیل برچسب ها به بردار\n",
        "# one-hot\n"
      ]
    },
    {
      "metadata": {
        "id": "VsnMBiltyZup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_oh_train =  keras.utils.to_categorical(Y_train,  5)\n",
        "Y_oh_test =  keras.utils.to_categorical(Y_test, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wljK_cS0z1oM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**بررسی داده های مدل  بر اساس ایندکسشان**"
      ]
    },
    {
      "metadata": {
        "id": "_2YzFV6Hya6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index = 20\n",
        "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rdCOjvnJ30fi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# GloVe\n",
        "\n",
        "تابع کمکی برای خواندن\n",
        "\n",
        "embedding\n",
        "\n",
        "از پیش آموزش داده شده\n",
        "\n",
        "[اطلاعات بیشتر در مورد گلوو](https://nlp.stanford.edu/projects/glove/)"
      ]
    },
    {
      "metadata": {
        "id": "A7L9V6F0BshD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yT1IZZBryeje",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, encoding=\"utf8\") as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "  \n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fb1fAdyNfoUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# تبدیل جمله به میانگین\n",
        "# Embedding \n",
        "# های کلمات آن\n",
        "\n",
        "**هر جمله را به کلمات تشکیل دهنده آن و سپس هر کلمه را به امبدینگ و در نهایت این بردارهای امبدینگ را میانگین خواهیم گرفت**."
      ]
    },
    {
      "metadata": {
        "id": "99sG2tbhy0pB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \n",
        "    # Split sentence into list of lower case words\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((50,))\n",
        "    \n",
        "    # average the word vectors. You can loop over the words in the list \"words\".\n",
        "    for w in words:\n",
        "        avg += word_to_vec_map[w]\n",
        "    avg = avg / len(words)\n",
        "    \n",
        "   \n",
        "    return avg\n",
        "  \n",
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "#print(\"avg = \", avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pNOcpwLEhwVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# به کار گیری مدل\n",
        "\n",
        "![alt text](https://cdn.pbrd.co/images/HXmaZjr.png)"
      ]
    },
    {
      "metadata": {
        "id": "NnmatYZNy5E4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def predict(X, Y, W, b, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data containing sentences, numpy array of shape (m, None)\n",
        "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
        "    \n",
        "    Returns:\n",
        "    pred -- numpy array of shape (m, 1) with your predictions\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    pred = np.zeros((m, 1))\n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        # Average words' vectors\n",
        "        avg = np.zeros((50,))\n",
        "        for w in words:\n",
        "            avg += word_to_vec_map[w]\n",
        "        avg = avg/len(words)\n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "        \n",
        "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
        "    \n",
        "    return pred\n",
        "\n",
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 401):\n",
        "    \"\"\"\n",
        "    Model to train word vector representations in numpy.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
        "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
        "    num_iterations -- number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
        "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
        "    b -- bias of the softmax layer, of shape (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 5                                 # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = keras.utils.to_categorical(Y, n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
        "        for i in range(m):                                # Loop over the training examples\n",
        "            \n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = -np.sum(Y_oh[i] * np.log(a))\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
        "\n",
        "    return pred, W, b\n",
        "  \n",
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "#print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTTLLMIvpgii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# بررسی عملکرد داده های تست"
      ]
    },
    {
      "metadata": {
        "id": "XSe738dZzT3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4V1ekSKHqRm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# confusion matrix\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "su9cGi1a4jfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    \n",
        "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "    \n",
        "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
        "    \n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_2JO5l34o_O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}